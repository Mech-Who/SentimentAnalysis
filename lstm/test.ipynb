{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "# MODIFY: model_from_yaml => model_from_json yaml格式因为有重大安全漏洞而被弃用\n",
    "from keras.models import Sequential, model_from_json\n",
    "# MODIFY: 从layers的子模块导入，修改为直接从layers导入\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# MODIFY: sklearn.cross_validation => sklearn.model_selection\n",
    "from sklearn.model_selection  import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg=pd.read_csv('../data/neg.csv',header=None,index_col=None)\n",
    "pos=pd.read_csv('../data/pos.csv',header=None,index_col=None, on_bad_lines='skip')\n",
    "neu=pd.read_csv('../data/neutral.csv', header=None, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        The thumbnails in the interface are too small ...\n",
       "1                                                  So good\n",
       "2        Please Take Out that Watch temporary thing it'...\n",
       "3        Netflix Apni sabhi Video ko sabhi Bhashao me D...\n",
       "4        Good collection of shows and films with some n...\n",
       "                               ...                        \n",
       "11878    Please update Netflix in which there is no scr...\n",
       "11879    Please provide update information. The slightl...\n",
       "11880    There is bug when i try share to instagram sto...\n",
       "11881    Yeah i think it's the best app sofar it's easi...\n",
       "11882    How can I delete one of the users? Please. Hel...\n",
       "Name: 0, Length: 11883, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112271,), 'I love this app')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADD: 添加填零操作，避免出现na被解析成float，导致后面的str操作失败\n",
    "pos.fillna('', inplace=True)\n",
    "neu.fillna('', inplace=True)\n",
    "neg.fillna('', inplace=True)\n",
    "combined = np.concatenate((pos[0], neu[0], neg[0]))\n",
    "combined.shape, combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112271,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos -> 1; neu -> 0; neg -> -1\n",
    "y = np.concatenate(\n",
    "    (np.ones(len(pos), dtype=int), \n",
    "    np.zeros(len(neu), dtype=int), \n",
    "    -1*np.ones(len(neg),dtype=int))\n",
    "    )\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.858 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#对句子经行分词，并去掉换行符\n",
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text\n",
    "\n",
    "# # 检测是否有不为str类型的数据\n",
    "# count = 0\n",
    "# for document in combined:\n",
    "#     if not isinstance(document, str):\n",
    "#         print(document, type(document), count)\n",
    "#     count += 1\n",
    "\n",
    "combined = tokenizer(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Word2vec model...\n"
     ]
    }
   ],
   "source": [
    "cpu_count = multiprocessing.cpu_count() # 4\n",
    "vocab_dim = 100\n",
    "n_iterations = 10  # ideally more..\n",
    "n_exposures = 10 # 所有频数超过10的词语\n",
    "window_size = 7\n",
    "n_epoch = 4\n",
    "input_length = 100\n",
    "maxlen = 100\n",
    "\n",
    "def create_dictionaries(model: Word2Vec=None,\n",
    "                        combined=None):\n",
    "    ''' \n",
    "    Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        # MODIFY: model.vocab => model.wv.key_to_index\n",
    "        gensim_dict.doc2bow(model.wv.key_to_index.keys(),\n",
    "                            allow_update=True)\n",
    "        #  freqxiao10->0 所以k+1\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()} # 所有频数超过10的词语的索引,(k->v)=>(v->k)\n",
    "        # MODIFY: model[word] => model.wv[word]\n",
    "        w2vec = {word: model.wv[word] for word in w2indx.keys()} # 所有频数超过10的词语的词向量, (word->model(word))\n",
    "\n",
    "        def parse_dataset(combined): # 闭包-->临时使用\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0) # freqxiao10->0\n",
    "                data.append(new_txt)\n",
    "            return data # word=>index\n",
    "        combined = parse_dataset(combined)\n",
    "        combined = sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec, combined\n",
    "    else:\n",
    "        print('No data provided...')\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "    # MODIFY: 新版本中size变成了vector_size，iter变成了epochs\n",
    "    model = Word2Vec(vector_size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     epochs=n_iterations)\n",
    "    model.build_vocab(combined) # input: list\n",
    "    # MODIFY: 新版本需要添加参数total_examples和epochs\n",
    "    # model.train(combined)\n",
    "    model.train(combined, total_examples=model.corpus_count, epochs=50)\n",
    "    model.save('../model/Word2vec_model.pkl')\n",
    "    index_dict, word_vectors, combined=create_dictionaries(model=model,combined=combined)\n",
    "    return index_dict, word_vectors,combined\n",
    "\n",
    "print('Training a Word2vec model...')\n",
    "index_dict, word_vectors, combined=word2vec_train(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n",
      "x_train.shape and y_train.shape:\n",
      "(89816, 100) (89816, 3)\n",
      "Defining a Simple Keras Model...\n",
      "Compiling the Model...\n",
      "Train...\n",
      "Epoch 1/4\n",
      "\u001b[1m2807/2807\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 42ms/step - accuracy: 0.7017 - loss: 0.8455\n",
      "Epoch 2/4\n",
      "\u001b[1m2807/2807\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 42ms/step - accuracy: 0.7801 - loss: 0.7692\n",
      "Epoch 3/4\n",
      "\u001b[1m2807/2807\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 42ms/step - accuracy: 0.7935 - loss: 0.7562\n",
      "Epoch 4/4\n",
      "\u001b[1m2807/2807\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 42ms/step - accuracy: 0.7978 - loss: 0.7523\n",
      "Evaluate...\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7897 - loss: 0.7592\n",
      "Test score: [0.7569557428359985, 0.7917612791061401]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)  # For Reproducibility\n",
    "sys.setrecursionlimit(1000000)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_data(index_dict, word_vectors, combined, y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim)) # 初始化 索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items(): # 从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=3) \n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=3)\n",
    "    # print x_train.shape,y_train.shape\n",
    "    return n_symbols, embedding_weights, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "##定义网络结构\n",
    "def train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test):\n",
    "    print('Defining a Simple Keras Model...')\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    # MODIFY: ouotput_dim => units\n",
    "    model.add(LSTM(units=50, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax')) # Dense=>全连接层,输出维度=1\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print('Compiling the Model...')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print(\"Train...\") # batch_size=32\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch,verbose=1)\n",
    "\n",
    "    print(\"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    # MODIFY: yaml格式改为json格式\n",
    "    # MODIFY: yaml_string = model.to_yaml() => json_string = model.to_json()\n",
    "    json_string = model.to_json()\n",
    "    with open('../model/lstm.json', 'w') as outfile:\n",
    "        # MODIFY: yaml.dump(yaml_string) => json.dump(json_string)\n",
    "        outfile.write( json.dumps(json_string) )\n",
    "    # MODIFY: 修改后缀： .h5 => .weights.h5\n",
    "    model.save_weights('../model/lstm.weights.h5')\n",
    "    print('Test score:', score)\n",
    "\n",
    "print('Setting up Arrays for Keras Embedding Layer...')\n",
    "n_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data(index_dict, word_vectors, combined, y)\n",
    "print(\"x_train.shape and y_train.shape:\")\n",
    "print(x_train.shape, y_train.shape)\n",
    "train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "预测\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "sys.setrecursionlimit(1000000)\n",
    "# define parameters\n",
    "maxlen = 100\n",
    "\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.key_to_index.keys(),\n",
    "                            allow_update=True)\n",
    "        #  freqxiao10->0 所以k+1\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引,(k->v)=>(v->k)\n",
    "        w2vec = {word: model.wv[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量, (word->model(word))\n",
    "\n",
    "        def parse_dataset(combined): # 闭包-->临时使用\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0) # freqxiao10->0\n",
    "                data.append(new_txt)\n",
    "            return data # word=>index\n",
    "        combined = parse_dataset(combined)\n",
    "        combined = sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec, combined\n",
    "    else:\n",
    "        print('No data provided...')\n",
    "\n",
    "\n",
    "def input_transform(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words=np.array(words).reshape(1,-1)\n",
    "    model=Word2Vec.load('../model/Word2vec_model.pkl')\n",
    "    _, _, combined=create_dictionaries(model,words)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def lstm_predict(string):\n",
    "    print('loading model......')\n",
    "    # MODIFY: yaml读取修改为json读取\n",
    "    # MODIFY: .yaml => .json\n",
    "    with open('../model/lstm.json', 'r') as f:\n",
    "        # MODIFY: yaml_string = yaml.load(f) => json_string = json.load(f)\n",
    "        json_string = json.load(f)\n",
    "    # MODIFY: model_from_yaml(yaml_string) => model_from_json(json_string)\n",
    "    model = model_from_json(json_string)\n",
    "\n",
    "    print('loading weights......')\n",
    "    model.load_weights('../model/lstm.weights.h5')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "    data=input_transform(string)\n",
    "    data.reshape(1,-1)\n",
    "    # print data\n",
    "    # MODIFY: predict_classes => predict\n",
    "    result = model.predict(data)\n",
    "    # ADD: 添加这一行，使得 result 和之前的 predict_classes 的结果保持一致\n",
    "    result = np.argmax(result, axis=1)\n",
    "    print(result) # [[1]]\n",
    "    if result[0]==1:\n",
    "        print(string,' positive')\n",
    "    elif result[0]==0:\n",
    "        print(string,' neural')\n",
    "    else:\n",
    "        print(string,' negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model......\n",
      "loading weights......\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f60ac14f130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step\n",
      "[1]\n",
      "书的质量还好，但是内容实在没意思。本以为会侧重心理方面的分析，但实际上是婚外恋内容。  positive\n"
     ]
    }
   ],
   "source": [
    "# string='酒店的环境非常好，价格也便宜，值得推荐'\n",
    "# string='手机质量太差了，傻逼店家，赚黑心钱，以后再也不会买了'\n",
    "# string = \"这是我看过文字写得很糟糕的书，因为买了，还是耐着性子看完了，但是总体来说不好，文字、内容、结构都不好\"\n",
    "# string = \"虽说是职场指导书，但是写的有点干涩，我读一半就看不下去了！\"\n",
    "string = \"书的质量还好，但是内容实在没意思。本以为会侧重心理方面的分析，但实际上是婚外恋内容。\"\n",
    "# string = \"不是太好\"\n",
    "# string = \"不错不错\"\n",
    "# string = \"非常好非常好！！\"\n",
    "# string = \"真的一般，没什么可以学习的\"\n",
    "\n",
    "lstm_predict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
